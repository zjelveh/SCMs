\subsection{Systematic Assessment of Specification Sensitivity using Machine Learning}

Now we introduce a systematic way for assessing the impact of these specification variations on estimates. We run specification curve analysis on the following choices: first, the full set of reasonable analytical choices, and then a more limited set of choices that are more theoretically justified for the Hogan analysis. To do so, we develop an R package called \texttt{SCMs} that automates this task by combining specification curve analysis with machine learning techniques to identify which analytical choices are most consequential for the results.

\subsubsection{Full Specification Space Analysis}

Our comprehensive specification curve analysis systematically explores the universe of reasonable analytical choices across multiple dimensions:

\textbf{Software Package Implementation:} We examine estimates from \texttt{Synth}, \texttt{AugSynth}, \texttt{scpi}, and \texttt{allsynth}, testing how different optimization routines and default settings affect results.

\textbf{Feature Weighting Methods:} We contrast optimized V-weights (as in the original \citet{abadie2010synthetic} method) versus uniform V-weights (the default in many modern packages). This tests whether allowing the algorithm to weight matching variables differently based on their predictive power affects treatment effect estimates.

\textbf{Bias Correction Approaches:} We compare traditional SCM (no bias correction) against modern extensions including Ridge regression, Lasso regression, OLS, and the augmented synthetic control method. These test the impact of attempting to improve upon traditional SCM when pre-period fit is imperfect through outcome model estimation.

\textbf{Matching Variable Sets:} We vary combinations of outcome variables, population controls, and clearance rates to test sensitivity to covariate inclusion choices that have been central to the Hogan-Kaplan debate.

\textbf{Data Aggregation Methods:} We examine per-period versus mean-aggregated approaches for both outcome and covariate variables, testing how temporal granularity in matching affects results.

\textbf{Constant Term Inclusion:} We test specifications with and without constant terms, which allow for controlled extrapolation by permitting parallel shifts in the counterfactual.

The results of this comprehensive analysis, shown in Figure \ref{fig:spec_curve_full}, reveal substantial variation in estimated treatment effects. Estimates range from near-zero increases in homicides to more than 100 additional homicides (+6.3 per 100K population), clearly spanning a range large enough to be policy-relevant. However, despite this variation, the analysis shows a consistent directional finding: approximately 90\% of all specifications yield positive treatment effects, with the distribution of estimates for Philadelphia notably shifted above zero compared to the placebo distribution centered around zero.

The median treatment effect across all specifications is positive but not statistically significant (p = 0.135), indicating that while the typical analytical choice points toward an increase in homicides, this finding is not statistically distinguishable from what might be observed for control cities by chance when considering the full analytical multiverse.

Our SHAP (SHapley Additive exPlanations) analysis, displayed in the bottom panel, identifies the specific analytical choices driving this variation. The specification curve plots also display predicted treatment effects from leave-one-out cross-validation alongside the actual estimated effects, providing insight into how well our machine learning model captures the relationship between specification choices and outcomes. This prediction accuracy gives us confidence in where the SHAP values should be most informative—the model's ability to predict treatment effects from specification features validates the interpretability of the SHAP decomposition. The analysis reveals that V-weight optimization is the most critical choice: using optimized V-weights (as in the original Synth package) has a large positive impact on treatment effect estimates (SHAP value: +13.2), while uniform V-weights systematically reduce the estimated effect (SHAP value: -13.3). Bias correction methods consistently push estimates downward, with traditional SCM (no bias correction) associated with much larger positive effects (SHAP value: +27.7), while Ridge (-10.8), Lasso (-7.5), OLS (-5.6), and augsynth (-7.5) all systematically reduce estimates. The choice of matching variables is also highly influential, with including all available variables (outcome, population, and clearances) associated with larger positive effects (SHAP value: +24.1) compared to matching only on outcome measures (SHAP value: -20.8).

\subsubsection{Refined Specification Analysis}

While the full specification space is valuable for understanding the complete analytical landscape, many combinations may not be theoretically well-motivated for the specific research context. We therefore conduct a refined analysis focusing on specifications that are more defensible given the nature of the intervention and the available data.

Our refined specification set narrows the analytical space by applying theoretically motivated constraints. Specifically, we restrict the analysis to: (1) \textbf{Weight constraints} using only simplex and penalized simplex (pensynth) methods, excluding less stable alternatives; (2) \textbf{Feature weights} limited to the optimized approach, which is more consistent with the original SCM methodology and allows proper utilization of multiple covariates; (3) \textbf{Data sample} restricted to the full donor pool rather than artificially limited subsets; (4) \textbf{Outcome models} including traditional SCM (none), augmented synthetic control (augsynth), Ridge regression, and OLS, but excluding less stable or theoretically problematic approaches; (5) \textbf{Constant terms} examining both inclusion and exclusion to test sensitivity to this methodological choice; and (6) \textbf{Feature aggregation} comparing three approaches: all variables as means, all variables per-period, and outcome-only per-period matching.

This refined set eliminates combinations that are either methodologically unstable or inconsistent with best practices for studying policy interventions in criminal justice settings, while retaining sufficient variation to test key theoretical disagreements in the literature.

The results of this refined analysis, presented in Figure \ref{fig:spec_curve_refined}, show a markedly different pattern. The distribution of treatment effects for Philadelphia becomes much more consistently positive, with the vast majority of specifications yielding effects well above zero. Most importantly, the statistical evidence becomes substantially stronger: the median treatment effect is now statistically significant (p = 0.014), indicating that the typical result from this more defensible set of analyses represents an effect for Philadelphia that is highly unlikely to be observed by chance.

However, the joint statistical evidence across the entire refined curve tells a more nuanced story. The Stouffer's p-value, which combines evidence across all specifications in the refined set, is 0.351. This indicates that while the median finding is statistically strong, there remains enough variability across the refined specification space that we cannot reject the joint null hypothesis of no effect across all justifiable models. The evidence for a treatment effect is therefore concentrated in the central tendency of the analytical choices but is not uniformly strong across the entire refined space.

The SHAP analysis for the refined specifications, again validated by strong predictive accuracy from leave-one-out cross-validation visible in the plots, confirms that the same fundamental choices remain most consequential. The choice of matching features continues to be paramount: using the full set of available matching variables (``Outcome Only'' feature set) has a large positive impact (SHAP value: +20.2), while matching only on outcome measures has a correspondingly large negative impact (SHAP value: -18.7). The weighting method remains influential, with the original approach associated with larger positive effects (SHAP value: +8.1) compared to penalized alternatives (SHAP value: -8.9). Bias correction continues to systematically reduce estimates, though the magnitudes are somewhat attenuated in this more restricted analytical space.

\subsubsection{Implications for the Hogan-Kaplan Debate}

This systematic specification curve analysis provides crucial insights into the source of disagreement between \citet{hogan2022prosecution} and \citet{kaplan2022prosecution}. Rather than representing arbitrary analytical choices or methodological errors, the conflicting results emerge from predictable consequences of different but individually defensible methodological decisions.

The analysis demonstrates that the debate is not fundamentally about whether an effect exists—the overwhelming majority of both the full and refined specification spaces point toward a positive treatment effect. Instead, the disagreement centers on the magnitude of an effect that consistently appears in the same direction. The key methodological levers identified through our SHAP analysis—V-weight optimization, bias correction approach, and matching variable selection—represent the primary sources of this magnitude variation.

Our framework moves beyond traditional robustness checks by providing a systematic method for understanding specification sensitivity. Rather than simply demonstrating that results change with different choices, the machine learning-enhanced specification curve analysis quantifies exactly how much each choice matters and identifies which decisions require the strongest theoretical justification. This transparency allows future researchers to focus their methodological discussions on the most consequential analytical decisions rather than getting lost in less important details.

The \texttt{SCMs} package we introduce automates this entire analytical pipeline, making it feasible for applied researchers to conduct comprehensive specification sensitivity analyses in their own work. This represents a methodological contribution that extends well beyond the specific Hogan-Kaplan debate, providing tools for more rigorous and transparent synthetic control analyses across diverse policy evaluation contexts.