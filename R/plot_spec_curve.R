#' Plot Specification Curve Using Direct Long Format Data
#'
#' @title Plot Specification Curve with Internal SHAP Computation and Filtering
#' @description Creates specification curve plots directly from long format data with integrated
#' SHAP analysis and flexible filtering capabilities. Key features:
#' - Internal SHAP computation using CatBoost (automatic when show_shap=TRUE)
#' - Specification filtering BEFORE SHAP computation and p-value calculation
#' - Perfect alignment between SHAP values and specifications via full_spec_id
#' - FAIL HARD error handling with specific guidance when requirements aren't met
#'
#' @param long_data Data.table or List. Long format data from spec_curve().
#'   Can be either the results data.table directly, or the full structured results
#'   (with results, abadie_inference, bootstrap_inference components).
#' @param name_treated_unit Character. Name of the treated unit.
#' @param outcomes Character vector or NULL. Names of the outcome variables to plot. If NULL, all outcomes in the data will be plotted.
#' @param normalize_outcomes Character. Method for normalizing treatment effects for comparability.
#'   Options: "none" (default), "percent_of_mean", "percent_of_preperiod", "standardized", "rmspe_ratio".
#'   - "percent_of_mean": (tau / mean(pre-period treated)) * 100
#'   - "percent_of_preperiod": (tau / final_preperiod_value) * 100
#'   - "standardized": tau / sd(control_effects)
#'   - "rmspe_ratio": post_RMSPE / pre_RMSPE (Abadie-style)
#' @param rmse_threshold Numeric. Threshold for root mean square error to filter results. Default is Inf.
#' @param shap_values Data.table or NULL. External SHAP values from run_catboost_shap_analysis().
#'   Should contain columns: unit, full_spec_id, feature_group, feature, shapley_value.
#'   If NULL and show_shap=TRUE, SHAP values will be computed internally. Default is NULL.
#' @param file_path_save Character or NA. File path to save the plot. If NA, plot is not saved. Default is NA.
#' @param width Numeric. Width of the saved plot in inches. Default is 6.
#' @param height Numeric. Height of the saved plot in inches. Default is 10.
#' @param show_pvalues Logical. Whether to include significance coloring for treatment effects.
#'   Uses Abadie p-values by default, or bootstrap p-values if available. Default is TRUE.
#' @param p_threshold Numeric. P-value threshold for significance coloring. Default is 0.05.
#' @param prefer_bootstrap_pvalues Logical. Whether to prefer bootstrap p-values over Abadie p-values
#'   when both are available. Default is FALSE (prefer Abadie).
#' @param test_statistic Character. Which test statistic p-values to use for coloring.
#'   Options: "rmse_ratio" (default), "treatment_effect", "normalized_te".
#'   Only applies to Abadie placebo inference which provides multiple test statistics.
#' @param null_distribution Character. The distribution to plot as the null/comparison.
#'   Options: "placebo" (default) or "bootstrap".
#'   - "placebo": Use placebo effects from control units as the null distribution.
#'   - "bootstrap": Use the bootstrapped null distribution for the treated unit.
#' @param crop_outliers Character or Numeric. Method for cropping outliers in Panel A.
#'   Options: "none" (default), "percentile", "iqr", "mad", or numeric vector c(ymin, ymax).
#'   - "percentile": Crop to 1st-99th percentile of treated unit effects
#'   - "iqr": Crop to Q1 - 1.5*IQR to Q3 + 1.5*IQR
#'   - "mad": Crop to median +/- 3*MAD (robust outlier detection)
#'   - c(ymin, ymax): Manual y-axis limits
#' @param sort_by Character. Method for sorting specifications on x-axis.
#'   Options: "tau" (default), "pvalue", "rmspe_ratio".
#'   - "tau": Sort by treatment effect magnitude (original behavior)
#'   - "pvalue": Sort by statistical significance (most significant first)
#'   - "rmspe_ratio": Sort by post/pre RMSPE ratio
#' @param filter_specs Named list or NULL. Filters to apply to specifications BEFORE SHAP computation and p-value calculation.
#'   Each element should be named by the feature group column and contain allowed values.
#'   Available filters: "constant" (TRUE/FALSE), "const" (simplex/lasso/ridge/pensynth), 
#'   "outcome_model" (none/augsynth/lasso/ridge/ols), "fw" (uniform/optimize), 
#'   "feat" (covariate aggregation labels), "data_sample" (all/most_similar).
#'   Examples: list(constant = "TRUE"), list(const = c("simplex", "lasso")), 
#'   list(constant = "TRUE", outcome_model = c("none", "augsynth")). Default is NULL (no filtering).
#' @param show_shap Logical. Whether to compute and display SHAP values. When TRUE:
#'   - If shap_values is provided: uses external SHAP values
#'   - If shap_values is NULL: computes SHAP values internally using CatBoost
#'   - Requires at least 3 unique specifications for internal computation
#'   - Automatically detects available specification features (outcome_model, const, fw, feat, data_sample, constant)
#'   Default is TRUE.
#' @param shap_config List. Configuration for SHAP computation and significance testing with elements:
#'   - compute_pvalues: Logical. Whether to calculate SHAP significance via placebo inference.
#'     Requires multi-unit SHAP data (treated_unit_only = FALSE in CatBoost config). Default is TRUE.
#'   - pvalue_type: Character. Method for SHAP significance testing: "absolute" or "signed".
#'     "absolute" ranks by |SHAP| values, "signed" uses two-sided rank-based test. Default is "absolute".
#' @param shap_label_type Character. How to display SHAP values in y-axis labels: "absolute" or "signed".
#'   - "absolute": Shows mean |SHAP| values, e.g., "ridge (0.123)"
#'   - "signed": Shows mean SHAP values with sign, e.g., "ridge (+0.089)" or "lasso (-0.045)"
#'   Default is "absolute".
#' @param show_predictions Logical. Whether to display predicted treatment effects from CatBoost models
#'   alongside actual treatment effects in Panel A. Uses leave-one-out cross-validation predictions
#'   for robust evaluation. Only available when SHAP is computed internally (show_shap=TRUE).
#'   Default is FALSE.
#' @param catboost_params List. Custom parameters for CatBoost model training. If NULL, uses defaults:
#'   list(loss_function='RMSE', iterations=100, depth=4, learning_rate=0.1, random_seed=42,
#'   verbose=0, bootstrap_type='Bayesian', bagging_temperature=1). Common parameters to modify:
#'   iterations (more = better fit but slower), depth (complexity), learning_rate (step size).
#'   Default is NULL.
#'
#' @return List containing:
#' \itemize{
#'   \item final_plot: ggplot object representing the complete specification curve with Panel A (treatment effects) and Panel B (feature groups/SHAP)
#'   \item panel_a: ggplot object for Panel A only (treatment effects)
#'   \item panel_b: ggplot object for Panel B only (feature groups/SHAP)
#'   \item plot_data_p1: data.table with Panel A plotting data including columns: Unit Name, Estimate, RMSE, Specification, unit_type, p_value (if available)
#'   \item plot_data_p2: data.table with Panel B plotting data including columns: Specification, feature_group, feature, shapley_value (if SHAP computed), shap_pvalue (if significance computed)
#'   \item computed_shap: Complete results from internal SHAP computation (NULL if external shap_values provided or show_shap=FALSE). 
#'     Contains: results (feature importance), shapley (SHAP values), predictions (model predictions), models (trained CatBoost models), config (SHAP configuration)
#'   \item spec_curve_pvals: List with specification curve-level p-values calculated on filtered data:
#'     median_tau_pvalues (median treatment effect ranks), stouffer_pvalues (Stouffer's Z-method ranks). NULL if no inference data available
#'   \item filtered_specs: Integer count of specifications remaining after filtering (before filtering if filter_specs=NULL)
#'   \item feature_groups_displayed: Character vector of feature groups shown in Panel B (only groups with variation in filtered data)
#' }
#'
#' @export
#'
#' @examples
#' \dontrun{
#' # ===== RECOMMENDED WORKFLOW: Internal SHAP Computation =====
#' 
#' # 1. Generate specification curve data
#' spec_results <- run_spec_curve_analysis(
#'   dataset = your_data,
#'   params = your_params,
#'   inference_type = "placebo"
#' )
#'
#' # 2. Basic plot with internal SHAP computation (most common use case)
#' plot_basic <- plot_spec_curve(
#'   long_data = spec_results,
#'   name_treated_unit = "TREATED_ID",
#'   show_shap = TRUE,  # Automatically computes SHAP internally
#'   shap_config = list(
#'     compute_pvalues = TRUE,
#'     pvalue_type = "absolute"
#'   )
#' )
#'
#' # ===== SPECIFICATION FILTERING EXAMPLES =====
#'
#' # Filter by constant terms only
#' plot_constant_only <- plot_spec_curve(
#'   long_data = spec_results,
#'   name_treated_unit = "TREATED_ID",
#'   filter_specs = list(constant = "TRUE"),
#'   show_shap = TRUE
#' )
#'
#' # Filter by multiple criteria (AND logic)
#' plot_filtered <- plot_spec_curve(
#'   long_data = spec_results,
#'   name_treated_unit = "TREATED_ID",
#'   filter_specs = list(
#'     constant = "TRUE",
#'     const = c("simplex", "lasso"),
#'     outcome_model = c("none", "augsynth")
#'   ),
#'   show_shap = TRUE
#' )
#'
#' # Compare specific constraint methods
#' plot_constraints <- plot_spec_curve(
#'   long_data = spec_results,
#'   name_treated_unit = "TREATED_ID",
#'   filter_specs = list(const = c("simplex", "lasso", "ridge")),
#'   show_shap = TRUE,
#'   shap_config = list(compute_pvalues = FALSE)  # Disable SHAP significance
#' )
#'
#' # ===== DIFFERENT TEST STATISTICS =====
#'
#' # Using treatment effect p-values
#' plot_treatment_effect <- plot_spec_curve(
#'   long_data = spec_results,
#'   name_treated_unit = "TREATED_ID",
#'   test_statistic = "treatment_effect",
#'   show_shap = TRUE
#' )
#'
#' # Using RMSE ratio p-values (default)
#' plot_rmse_ratio <- plot_spec_curve(
#'   long_data = spec_results,
#'   name_treated_unit = "TREATED_ID",
#'   test_statistic = "rmse_ratio",
#'   show_shap = TRUE
#' )
#'
#' # ===== SHAP SIGNIFICANCE TESTING =====
#'
#' # SHAP with signed significance testing
#' plot_shap_signed <- plot_spec_curve(
#'   long_data = spec_results,
#'   name_treated_unit = "TREATED_ID",
#'   show_shap = TRUE,
#'   shap_config = list(
#'     compute_pvalues = TRUE,
#'     pvalue_type = "signed"  # Two-sided SHAP significance
#'   )
#' )
#'
#' # ===== EXTERNAL SHAP WORKFLOW (Advanced) =====
#'
#' # For advanced users who want external control over SHAP computation
#' shap_config_external <- create_catboost_config(
#'   dataset_name = "example",
#'   treated_unit_name = "TREATED_ID",
#'   treated_unit_only = FALSE  # Multi-unit for significance testing
#' )
#' external_shap <- run_catboost_shap_analysis(spec_results, shap_config_external)
#'
#' plot_external_shap <- plot_spec_curve(
#'   long_data = spec_results,
#'   name_treated_unit = "TREATED_ID",
#'   shap_values = external_shap$shapley,  # Provide external SHAP
#'   show_shap = TRUE
#' )
#'
#' # ===== OTHER OPTIONS =====
#'
#' # Disable SHAP entirely
#' plot_no_shap <- plot_spec_curve(
#'   long_data = spec_results,
#'   name_treated_unit = "TREATED_ID",
#'   show_shap = FALSE
#' )
#'
#' # Crop outliers and sort by p-values
#' plot_customized <- plot_spec_curve(
#'   long_data = spec_results,
#'   name_treated_unit = "TREATED_ID",
#'   show_shap = TRUE,
#'   crop_outliers = "percentile",
#'   sort_by = "pvalue",
#'   normalize_outcomes = "standardized"
#' )
#'
#' # ===== EXTRACTING INDIVIDUAL PANELS =====
#'
#' # Extract and display only Panel A (treatment effects)
#' panel_a <- extract_panel_a(plot_customized)
#' 
#' # Extract and save Panel B (specification features/SHAP)
#' extract_panel_b(plot_customized, file_path = "shap_features.png", width = 10, height = 8)
#'
#' }
#' @importFrom ggtext element_markdown
plot_spec_curve <- function(
    long_data,
    name_treated_unit,
    outcomes = NULL,
    normalize_outcomes = "none",
    rmse_threshold = Inf,
    shap_values = NULL,
    file_path_save = NA,
    width = 6,
    height = 10,
    show_pvalues = TRUE,
    p_threshold = 0.05,
    prefer_bootstrap_pvalues = FALSE,
    test_statistic = "rmse_ratio",
    null_distribution = "placebo",
    crop_outliers = "none",
    sort_by = "tau",
    filter_specs = NULL,
    show_shap = TRUE,
    shap_config = list(
        compute_pvalues = TRUE,
        pvalue_type = "absolute"
    ),
    shap_label_type = "absolute",
    show_predictions = FALSE,
    catboost_params = NULL
) {

    # Libraries imported via NAMESPACE
    
    # Define color constants to avoid redundancy
    SHAP_COLORS <- list(
        red = "#CA0020",      # Red for negative SHAP
        gray = "#969696",     # Gray for near-zero SHAP
        blue = "#0571B0",     # Blue for positive SHAP
        light_gray = "#F0F0F0" # Light gray for low absolute SHAP
    )
    
    # Helper function to map SHAP values to colors using same scale as legend
    map_shap_to_color <- function(shap_values, shap_label_type, all_shap_values = NULL) {
        if (is.null(all_shap_values)) {
            all_shap_values <- shap_values
        }
        
        if (shap_label_type == "absolute") {
            # Map absolute values using same scale as gradient: light_gray to blue
            abs_values <- abs(shap_values)
            abs_range <- range(abs(all_shap_values), na.rm = TRUE)
            if (abs_range[2] == abs_range[1]) {
                return(rep(SHAP_COLORS$gray, length(shap_values)))
            }
            # Normalize to 0-1 range
            normalized <- (abs_values - abs_range[1]) / (abs_range[2] - abs_range[1])
            # Interpolate between light_gray and blue
            colors <- grDevices::colorRamp(c(SHAP_COLORS$light_gray, SHAP_COLORS$blue))(normalized)
            return(grDevices::rgb(colors[,1], colors[,2], colors[,3], maxColorValue = 255))
        } else {
            # Map signed values using same scale as gradient2: red to gray to blue (midpoint = 0)
            shap_range <- range(all_shap_values, na.rm = TRUE)
            if (shap_range[2] == shap_range[1]) {
                return(rep(SHAP_COLORS$gray, length(shap_values)))
            }
            
            # Create symmetric range around 0 for proper gradient2 behavior
            max_abs <- max(abs(shap_range), na.rm = TRUE)
            
            # Vectorized color mapping for signed values
            result_colors <- character(length(shap_values))
            
            for (i in seq_along(shap_values)) {
                val <- shap_values[i]
                if (is.na(val)) {
                    result_colors[i] <- SHAP_COLORS$gray
                } else if (val == 0) {
                    result_colors[i] <- SHAP_COLORS$gray
                } else if (val < 0) {
                    # Map negative values from red to gray
                    proportion <- abs(val) / max_abs
                    color_matrix <- grDevices::colorRamp(c(SHAP_COLORS$gray, SHAP_COLORS$red))(proportion)
                    result_colors[i] <- grDevices::rgb(color_matrix[1], color_matrix[2], color_matrix[3], maxColorValue = 255)
                } else {
                    # Map positive values from gray to blue
                    proportion <- val / max_abs
                    color_matrix <- grDevices::colorRamp(c(SHAP_COLORS$gray, SHAP_COLORS$blue))(proportion)
                    result_colors[i] <- grDevices::rgb(color_matrix[1], color_matrix[2], color_matrix[3], maxColorValue = 255)
                }
            }
            
            return(result_colors)
        }
    }

    # Helper function for filtering specifications
    apply_spec_filters <- function(data, filter_specs) {
        if (!data.table::is.data.table(data)) {
            data <- data.table::as.data.table(data)
        }
        
        filtered_data <- data.table::copy(data)
        
        # Apply each filter sequentially
        for (filter_col in names(filter_specs)) {
            filter_values <- filter_specs[[filter_col]]
            
            # Check if the filter column exists in the data
            if (!filter_col %in% names(filtered_data)) {
                warning(paste("Filter column", filter_col, "not found in data. Available columns:",
                             paste(names(filtered_data), collapse = ", ")))
                next
            }
            
            # Apply the filter
            initial_rows <- nrow(filtered_data)
            filtered_data <- filtered_data[get(filter_col) %in% filter_values]
            final_rows <- nrow(filtered_data)
            
            # Provide feedback about filtering effect
            message(paste("Filter", filter_col, ":", initial_rows, "->", final_rows, "specifications"))
            
            # Check if filtering eliminated all data
            if (nrow(filtered_data) == 0) {
                stop(paste("Filter", filter_col, "eliminated all specifications. No data remaining."))
            }
        }
        
        message(paste("Total specifications after filtering:", nrow(filtered_data)))
        return(filtered_data)
    }

    # Input validation
    valid_test_statistics <- c("rmse_ratio", "treatment_effect", "normalized_te")
    if (!test_statistic %in% valid_test_statistics) {
        stop("test_statistic must be one of: ", paste(valid_test_statistics, collapse = ", "))
    }
    # Validate filter_specs structure
    if (!is.null(filter_specs) && !is.list(filter_specs)) {
        stop("filter_specs must be a named list or NULL")
    }
    
    # Validate shap_config structure
    if (!is.list(shap_config)) {
        stop("shap_config must be a list")
    }
    
    # Set default shap_config values
    if (is.null(shap_config$compute_pvalues)) {
        shap_config$compute_pvalues <- TRUE
    }
    if (is.null(shap_config$pvalue_type)) {
        shap_config$pvalue_type <- "absolute"
    }
    
    # Validate shap_config$pvalue_type
    valid_shap_pvalue_types <- c("absolute", "signed")
    if (!shap_config$pvalue_type %in% valid_shap_pvalue_types) {
        stop("shap_config$pvalue_type must be one of: ", paste(valid_shap_pvalue_types, collapse = ", "))
    }
    
    # Validate shap_label_type
    valid_shap_label_types <- c("absolute", "signed")
    if (!shap_label_type %in% valid_shap_label_types) {
        stop("shap_label_type must be one of: ", paste(valid_shap_label_types, collapse = ", "))
    }

    # Input validation and data extraction
    if (is.list(long_data) && "results" %in% names(long_data)) {
        # Handle structured results from spec_curve
        main_data <- data.table::as.data.table(long_data$results)

        # Merge inference results for the specified test statistic
        p_values_key <- paste0("p_values_", test_statistic)
        test_stats_key <- paste0("test_statistics_", test_statistic)


        if (!is.null(long_data$abadie_inference[[p_values_key]])) {
            abadie_pvals <- long_data$abadie_inference[[p_values_key]]
            # limit to treated unit
            abadie_pvals = abadie_pvals[unit_type=='treated']
            main_data = merge(main_data, abadie_pvals[, .(full_spec_id, p_value)],
                by = "full_spec_id", all.x = TRUE)
        }

        # Merge test statistics values using unit-level keys to avoid cartesian joins
        if (!is.null(long_data$abadie_inference[[test_stats_key]])) {
            test_stats_data <- long_data$abadie_inference[[test_stats_key]]
            # Include unit_type to ensure unique merging (avoids cartesian joins)
            merge_cols <- intersect(c("full_spec_id", "unit_name", "unit_type"), names(test_stats_data))
            if (length(merge_cols) >= 2) {  # Need at least full_spec_id + unit identifier
                # Get the appropriate column name for the test statistic value
                if ("test_statistic_value" %in% names(test_stats_data)) {
                    value_col <- "test_statistic_value"
                } else if ("post_pre_ratio" %in% names(test_stats_data)) {
                    value_col <- "post_pre_ratio"
                } else {
                    value_col <- NULL
                }

                if (!is.null(value_col)) {
                    main_data = merge(main_data, test_stats_data[, c(merge_cols, value_col), with = FALSE],
                        by = merge_cols, all.x = TRUE)
                    # Rename to standardized column name for backward compatibility
                    if (value_col == "test_statistic_value" && test_statistic == "rmse_ratio") {
                        setnames(main_data, "test_statistic_value", "post_pre_ratio")
                    }
                }
            } else {
                warning("Insufficient merge columns for test statistics - skipping merge")
            }
        }

        if (!is.null(long_data$bootstrap_inference$p_values)) {
            bootstrap_pvals <- long_data$bootstrap_inference$p_values
            main_data = merge(main_data, bootstrap_pvals[, .(full_spec_id, p_value_two_tailed )],
                by = "full_spec_id", all.x = TRUE)
        }


        # Add bootstrap iteration data for null distribution plotting
        if (!is.null(long_data$bootstrap_inference$iteration_data)) {
            bootstrap_iteration_data <- long_data$bootstrap_inference$iteration_data

            # Check if bootstrap iteration data exists and has data
            if (length(bootstrap_iteration_data) > 0) {
                # Combine all outcome models' bootstrap data
                bootstrap_combined <- rbindlist(bootstrap_iteration_data, fill = TRUE)

                if (nrow(bootstrap_combined) > 0) {
                    # Add necessary columns to match main_data structure
                    if (!"rmse" %in% names(bootstrap_combined)) {
                        bootstrap_combined[, rmse := NA_real_]
                    }

                    # Bootstrap data needs specification metadata - get unique spec metadata from main_data
                    spec_metadata <- unique(main_data[, .(full_spec_id, outcome, outcome_model, const, fw, feat, data_sample)])

                    # For each bootstrap row, we need to replicate it across all specifications for that outcome_model
                    bootstrap_expanded <- list()
                    for (i in 1:nrow(bootstrap_combined)) {
                        boot_row <- bootstrap_combined[i]
                        matching_specs <- spec_metadata[outcome_model == boot_row$outcome_model]

                        if (nrow(matching_specs) > 0) {
                            # Replicate this bootstrap observation for each matching specification
                            expanded_rows <- boot_row[rep(1, nrow(matching_specs))]
                            expanded_rows <- cbind(expanded_rows, matching_specs)
                            bootstrap_expanded[[i]] <- expanded_rows
                        }
                    }

                    if (length(bootstrap_expanded) > 0) {
                        bootstrap_final <- rbindlist(bootstrap_expanded, fill = TRUE)
                        main_data <- rbindlist(list(main_data, bootstrap_final), fill = TRUE)
                    }
                }
            }
        }

        # Keep main_data as the processed results data
    } else {
        # Handle direct data.table input (not structured from spec_curve)
        if (!data.table::is.data.table(long_data)) {
            long_data <- data.table::as.data.table(long_data)
        }
        main_data <- long_data
    }

    # Check required columns
    required_cols <- c("unit_name", "tau", "post_period", "full_spec_id")
    missing_cols <- setdiff(required_cols, names(main_data))
    if (length(missing_cols) > 0) {
        stop(paste("Missing required columns in main_data:", paste(missing_cols, collapse = ", ")))
    }

    # Check if p-value data is available
    has_abadie_pvalues <- "p_value" %in% names(main_data)
    has_bootstrap_pvalues <- "p_value_two_tailed" %in% names(main_data)
    has_pvalues <- has_abadie_pvalues || has_bootstrap_pvalues


    # Determine which p-values to use
    # When using bootstrap null distribution, prefer bootstrap p-values
    if (null_distribution == "bootstrap" && has_bootstrap_pvalues) {
        use_bootstrap_pvalues <- TRUE
    } else if (prefer_bootstrap_pvalues && has_bootstrap_pvalues) {
        use_bootstrap_pvalues <- TRUE
    } else if (!has_abadie_pvalues && has_bootstrap_pvalues) {
        use_bootstrap_pvalues <- TRUE
    } else {
        use_bootstrap_pvalues <- FALSE
    }

    p_value_column <- if (use_bootstrap_pvalues) "p_value_two_tailed" else "p_value"

    # Apply filtering pipeline in proper order
    sc_results_df <- main_data
    
    # 1. Filter by outcomes (if specified)
    if (!is.null(outcomes) && "outcome" %in% names(sc_results_df)) {
        sc_results_df <- sc_results_df[outcome %in% outcomes]
    }

    # 2. Filter by RMSE if threshold provided
    if ("rmse" %in% names(sc_results_df) && rmse_threshold < Inf) {
        sc_results_df <- sc_results_df[rmse < rmse_threshold]
    }
    
    # 3. Apply specification filters BEFORE p-value calculations
    if (!is.null(filter_specs)) {
        sc_results_df <- apply_spec_filters(sc_results_df, filter_specs)
    }

    if (nrow(sc_results_df) == 0) {
        stop("No data remaining after filtering")
    }

    # Calculate average effects for plotting (post-treatment period only)
    # Define common grouping variables for aggregation
    agg_grouping_vars <- c('unit_name', 'unit_type', 'full_spec_id', 'outcome',
                          'outcome_model', 'const', 'fw', 'feat', 'data_sample', 'constant')
    
    # Perform aggregation with conditional columns
    if ("post_pre_ratio" %in% names(sc_results_df)) {
        average_effect_df <- sc_results_df[post_period == TRUE, 
                                         list(tau = mean(tau, na.rm = TRUE),
                                              rmse = mean(rmse, na.rm = TRUE),
                                              post_pre_ratio = mean(post_pre_ratio, na.rm = TRUE)), 
                                         by = agg_grouping_vars]
    } else {
        average_effect_df <- sc_results_df[post_period == TRUE, 
                                         list(tau = mean(tau, na.rm = TRUE),
                                              rmse = mean(rmse, na.rm = TRUE)), 
                                         by = agg_grouping_vars]
    }


    # Normalize outcomes if requested
    if (normalize_outcomes != "none") {

        if (normalize_outcomes == "standardized") {
            # Standardize by comparison unit standard deviation (control or bootstrap)
            comparison_units <- average_effect_df[unit_type %in% c("control", "bootstrap")]
            sd_outcome <- sd(comparison_units$tau, na.rm = TRUE)
            if (sd_outcome > 0) {
                average_effect_df[, tau := tau / sd_outcome]
            }
            y_label <- "Standardized Treatment Effect"

        } else if (normalize_outcomes == "percent_of_mean") {
            # For percentage normalization, we need the outcome scale
            # Since tau = actual - synthetic, use RMSE as a proxy for outcome scale
            treated_rmse <- average_effect_df[unit_name == name_treated_unit, mean(rmse, na.rm = TRUE)]
            if (!is.na(treated_rmse) && treated_rmse > 0) {
                # Normalize by typical outcome scale (approximated by RMSE)
                average_effect_df[unit_name == name_treated_unit, tau := (tau / treated_rmse) * 100]
            }
            y_label <- "Treatment Effect (% of Outcome Scale)"

        } else if (normalize_outcomes == "percent_of_preperiod") {
            # Similar approach using RMSE as scale
            treated_rmse <- average_effect_df[unit_name == name_treated_unit, mean(rmse, na.rm = TRUE)]
            if (!is.na(treated_rmse) && treated_rmse > 0) {
                average_effect_df[unit_name == name_treated_unit, tau := (tau / treated_rmse) * 100]
            }
            y_label <- "Treatment Effect (% of Outcome Scale)"
        } else if (normalize_outcomes == "rmspe_ratio") {
            # Use post/pre RMSPE ratio if available
            if ("post_pre_ratio" %in% names(average_effect_df)) {
                average_effect_df[, tau := post_pre_ratio]
            }
            y_label <- "Post/Pre RMSPE Ratio"
        } else {
            y_label <- "Average Treatment Effect"
        }
    } else {
        y_label <- "Average Treatment Effect"
    }

    # Calculate y-axis limits for outlier cropping
    y_limits <- NULL
    if (crop_outliers != "none") {
        treated_effects <- average_effect_df[unit_name == name_treated_unit, tau]

        if (is.numeric(crop_outliers) && length(crop_outliers) == 2) {
            # Manual limits
            y_limits <- crop_outliers

        } else if (crop_outliers == "percentile") {
            # 5th to 95th percentile
            y_limits <- quantile(treated_effects, c(0.01, 0.99), na.rm = TRUE)

        } else if (crop_outliers == "iqr") {
            # Interquartile range with 1.5*IQR extension
            q1 <- quantile(treated_effects, 0.25, na.rm = TRUE)
            q3 <- quantile(treated_effects, 0.75, na.rm = TRUE)
            iqr <- q3 - q1
            y_limits <- c(q1 - 1.5 * iqr, q3 + 1.5 * iqr)

        } else if (crop_outliers == "mad") {
            # Median +/- 3 * Median Absolute Deviation
            med <- median(treated_effects, na.rm = TRUE)
            mad_val <- mad(treated_effects, na.rm = TRUE)
            y_limits <- c(med - 3 * mad_val, med + 3 * mad_val)
        }
    }

    # --- 1. Data Preparation ---

    # A. Prepare data for Panel A and create numbered specifications
    panel_a_data <- copy(average_effect_df)

    # Create specification numbering based on treated unit effects (sorted by normalized tau)
    treated_specs <- panel_a_data[unit_name == name_treated_unit][order(tau)]
    treated_specs[, Specification := 1:.N]
    spec_mapping <- treated_specs[, .(full_spec_id, Specification)]

    # Apply specification numbering to all data
    panel_a_data <- merge(panel_a_data, spec_mapping, by = "full_spec_id", all.x = TRUE)
    setnames(panel_a_data,
             old = c("unit_name", "tau", "rmse"),
             new = c("Unit Name", "Estimate", "RMSE"))

    # Add p-values if requested
    if (has_pvalues && show_pvalues) {
        p_value_data <- unique(sc_results_df[unit_name == name_treated_unit & !is.na(get(p_value_column)),
                                             .(full_spec_id, p_value = get(p_value_column))])
        p_value_data <- merge(p_value_data, spec_mapping, by = "full_spec_id", all.x = TRUE)

        panel_a_data <- merge(panel_a_data, p_value_data[, .(Specification, p_value)], by = "Specification", all.x = TRUE)
    }
    


    # B. Prepare data for Panel B (SHAP & Specification Details)
    if ("post_pre_ratio" %in% names(average_effect_df)) {
        average_effect_df[, post_pre_ratio := NULL]
    }
    
    # Convert constant column to character to avoid melt type warning
    if ("constant" %in% names(average_effect_df)) {
        average_effect_df[, constant := as.character(constant)]
    }
    
    panel_b_data <- melt(average_effect_df[unit_name == name_treated_unit],
                         id.vars = c('unit_name', 'full_spec_id', 'tau', 'rmse', 'unit_type'),
                         variable.name = 'feature_group', value.name = 'feature')

    # Apply the same specification numbering
    panel_b_data <- merge(panel_b_data, spec_mapping, by = "full_spec_id", all.x = TRUE)

    # Handle SHAP computation and merging
    computed_shap <- NULL
    
    # Check if we need to compute SHAP internally
    if (show_shap && is.null(shap_values)) {
        message("Internal SHAP computation requested. Running CatBoost SHAP analysis...")
        
        # Create default configuration for internal SHAP computation
        # Use the most common specifications for internal analysis
        default_spec_features <- c("outcome_model", "const", "fw", "feat", "data_sample")
        
        # Add 'constant' to spec features if it exists in the data and has variation
        if ("constant" %in% names(sc_results_df)) {
            constant_variation <- data.table::uniqueN(sc_results_df$constant) > 1
            if (constant_variation) {
                default_spec_features <- c(default_spec_features, "constant")
                message("Adding 'constant' to SHAP features - detected variation in constant terms")
            }
        }
        
        # Filter to available spec features only
        available_spec_features <- intersect(default_spec_features, names(sc_results_df))
        
        if (length(available_spec_features) == 0) {
            stop("Internal SHAP computation failed: No valid specification features found in data. ",
                 "Available columns: ", paste(names(sc_results_df), collapse = ", "), ". ",
                 "Either provide shap_values parameter or ensure data contains specification features.")
        }
        
        # Detect outcome filter from filtered data  
        outcome_filter <- NULL
        if ("outcome" %in% names(sc_results_df)) {
            unique_outcomes <- unique(sc_results_df$outcome)
            if (length(unique_outcomes) == 1) {
                outcome_filter <- unique_outcomes[1]
            }
        }
        
        # Create configuration for internal SHAP computation
        shap_config_internal <- create_catboost_config(
            dataset_name = paste0("internal_shap_", Sys.time()),
            treated_unit_name = name_treated_unit,
            outcome_filter = outcome_filter,
            spec_features = available_spec_features,
            treated_unit_only = TRUE,  # Default to treated unit only for efficiency
            catboost_params = catboost_params  # Pass custom CatBoost parameters
        )
        
        # Create long format data structure expected by run_catboost_shap_analysis
        # Add spec_number for proper alignment 
        long_format_data <- copy(sc_results_df)
        
        # Check minimum specifications required for CatBoost
        treated_unit_specs <- unique(sc_results_df[unit_name == name_treated_unit, full_spec_id])
        n_unique_specs <- length(treated_unit_specs)
        
        if (n_unique_specs < 3) {
            stop("Internal SHAP computation failed: Insufficient specifications for CatBoost analysis. ",
                 "Found ", n_unique_specs, " unique specifications, but at least 3 are required. ",
                 "Either provide external shap_values, set show_shap=FALSE, or run with more specification variations.")
        }
        
        # Add spec_number if not present (based on treated unit ordering like in plot creation)
        if (!"spec_number" %in% names(long_format_data)) {
            treated_specs <- long_format_data[unit_name == name_treated_unit & post_period == TRUE]
            if (nrow(treated_specs) > 0) {
                treated_specs_ordered <- treated_specs[order(tau)]
                treated_specs_ordered[, spec_number := 1:.N]
                spec_number_mapping <- treated_specs_ordered[, .(full_spec_id, spec_number)]
                
                # Apply to all data
                long_format_data <- merge(long_format_data, spec_number_mapping, 
                                        by = "full_spec_id", all.x = TRUE)
            } else {
                stop("Internal SHAP computation failed: No treated unit data found for spec_number creation")
            }
        }
        
        # Run internal SHAP analysis
        shap_results_internal <- tryCatch({
            run_catboost_shap_analysis(long_format_data, shap_config_internal)
        }, error = function(e) {
            stop("Internal SHAP computation failed: ", e$message, ". ",
                 "Either provide shap_values parameter or set show_shap=FALSE.")
        })
        
        # Extract SHAP values and store computed results
        if (!is.null(shap_results_internal) && !is.null(shap_results_internal$shapley)) {
            shap_values <- shap_results_internal$shapley
            computed_shap <- shap_results_internal
            message("Internal SHAP computation completed successfully. ", 
                    nrow(shap_values), " SHAP observations computed.")
        } else {
            stop("Internal SHAP computation failed: No SHAP values returned. ",
                 "Either provide shap_values parameter or set show_shap=FALSE.")
        }
    }
    
    # Merge SHAP values if available (either provided or computed)
    if (!is.null(shap_values)) {
        # Validate SHAP data structure - FAIL HARD if incorrect
        required_shap_cols <- c('unit', 'full_spec_id', 'feature_group', 'feature', 'shapley_value')
        missing_shap_cols <- setdiff(required_shap_cols, names(shap_values))
        if (length(missing_shap_cols) > 0) {
            stop("Invalid SHAP data structure. Missing required columns: ", 
                 paste(missing_shap_cols, collapse = ", "), ". ",
                 "SHAP data must contain columns: ", paste(required_shap_cols, collapse = ", "))
        }
        
        # Check for multi-unit SHAP data and calculate significance if requested
        unique_units <- unique(shap_values$unit)
        has_control_shaps <- length(unique_units) > 1 && any(unique_units != name_treated_unit)

        if (has_control_shaps && shap_config$compute_pvalues) {
            message("Found SHAP values for ", length(unique_units), " units (",
                    sum(unique_units != name_treated_unit), " controls). Calculating SHAP significance using '", shap_config$pvalue_type, "' method...")
            message("To disable SHAP significance testing, set shap_config$compute_pvalues = FALSE")

            # Calculate SHAP significance on filtered data
            shap_significance <- calculate_shap_significance(shap_values, name_treated_unit, pvalue_method = shap_config$pvalue_type)
            # Merge significance data with SHAP values
            shap_values <- merge(shap_values, shap_significance,
                                 by = c("full_spec_id", "feature_group"), all.x = TRUE)

            # Provide user feedback about significance results
            treated_significance <- shap_significance[!is.na(shap_pvalue)]
            if (nrow(treated_significance) > 0) {
                n_significant <- sum(treated_significance$shap_pvalue < 0.05, na.rm = TRUE)
                n_marginal <- sum(treated_significance$shap_pvalue >= 0.05 & treated_significance$shap_pvalue < 0.10, na.rm = TRUE)
                total_tests <- nrow(treated_significance)
                message("SHAP significance: ", n_significant, " significant (p<0.05), ",
                        n_marginal, " marginal (p<0.10) out of ", total_tests, " tests")
            }
        } else if (has_control_shaps && !shap_config$compute_pvalues) {
            message("Multi-unit SHAP data found but significance testing disabled by shap_config$compute_pvalues = FALSE")
        }

        # Debug: Show data structures before merging
        message("DEBUG: SHAP merging diagnostics...")
        message("Panel B data structure (first 5 rows for treated unit):")
        panel_b_treated <- panel_b_data[unit_name == name_treated_unit][1:min(5, .N)]
        if (nrow(panel_b_treated) > 0) {
            message("  Feature groups: ", paste(unique(panel_b_treated$feature_group), collapse = ", "))
            message("  Sample features: ", paste(head(panel_b_treated$feature, 5), collapse = ", "))
        }
        
        message("SHAP values structure (first 5 rows for treated unit):")
        shap_treated <- shap_values[unit == name_treated_unit][1:min(5, .N)]
        if (nrow(shap_treated) > 0) {
            message("  Feature groups: ", paste(unique(shap_treated$feature_group), collapse = ", "))
            message("  Sample features: ", paste(head(shap_treated$feature, 5), collapse = ", "))
        }

        # FIXED: Aggregate SHAP values to specification level before merging
        # SHAP values are at individual feature level, but panel_b needs them at specification level
        message("Aggregating SHAP values to specification level...")
        
        # Aggregate SHAP values by specification (full_spec_id) and feature_group 
        # This sums up all SHAP contributions within each specification dimension
        shap_aggregated <- shap_values[, .(
            shapley_value = sum(shapley_value, na.rm = TRUE),
            n_features = .N
        ), by = .(unit, full_spec_id, feature_group)]
        
        message("SHAP aggregation: ", nrow(shap_values), " individual feature SHAP values -> ", 
                nrow(shap_aggregated), " specification-level SHAP values")
        
        # For each aggregated SHAP value, we need to determine the corresponding 'feature' value
        # This is the actual categorical value for that specification dimension
        panel_b_treated <- panel_b_data[unit_name == name_treated_unit]
        
        if (nrow(panel_b_treated) > 0) {
            # Create lookup for feature values by full_spec_id and feature_group
            feature_lookup <- unique(panel_b_treated[, .(full_spec_id, feature_group, feature)])
            
            # Merge feature values into aggregated SHAP data
            shap_aggregated <- merge(shap_aggregated, feature_lookup,
                                   by = c("full_spec_id", "feature_group"),
                                   all.x = TRUE)
        }
        
        # Perform the merge with aggregated SHAP values
        panel_b_data_before_merge <- copy(panel_b_data)
        panel_b_data <- merge(panel_b_data, shap_aggregated,
                              by.x = c('unit_name', 'full_spec_id', 'feature_group', 'feature'),
                              by.y = c('unit', 'full_spec_id', 'feature_group', 'feature'),
                              all.x = TRUE)
        
        # Debug: Check merge success rate
        treated_rows_before <- nrow(panel_b_data_before_merge[unit_name == name_treated_unit])
        treated_rows_with_shap <- nrow(panel_b_data[unit_name == name_treated_unit & !is.na(shapley_value)])
        merge_success_rate <- treated_rows_with_shap / treated_rows_before * 100
        message("SHAP merge success: ", treated_rows_with_shap, "/", treated_rows_before, 
                " (", round(merge_success_rate, 1), "%) rows got SHAP values")
        
        if (merge_success_rate < 50) {
            warning("Low SHAP merge success rate (", round(merge_success_rate, 1), "%). ",
                    "Feature name mismatch may still exist between panel_b_data and aggregated SHAP values.")
        } else {
            message("SHAP merging successful - aggregation approach resolved feature granularity mismatch")
        }
    }

    setnames(panel_b_data, c('tau', 'unit_name', 'rmse'),
             c('Estimate', 'Unit Name', 'RMSE'))

    # Add predicted treatment effects if requested and available (after SHAP computation)
    if (show_predictions && !is.null(computed_shap) && !is.null(computed_shap$predictions)) {
        # Extract predictions for treated unit
        predictions_data <- computed_shap$predictions[unit == name_treated_unit]
        
        if (nrow(predictions_data) > 0 && "predicted_loo" %in% names(predictions_data)) {
            # Merge predictions with specification mapping
            if ("full_spec_id" %in% names(predictions_data)) {
                pred_with_spec <- merge(predictions_data[, list(full_spec_id, predicted_loo)], 
                                      spec_mapping, by = "full_spec_id", all.x = TRUE)
                
                # Add to panel data
                panel_a_data <- merge(panel_a_data, pred_with_spec[, list(Specification, Predicted = predicted_loo)], 
                                    by = "Specification", all.x = TRUE)
            }
        }
    }

    # C. Re-sort specifications if requested (default is by tau, which is already done)
    if (sort_by != "tau") {
        # Create new ordering based on sort_by parameter
        treated_data <- panel_a_data[unit_type == "treated"]

        if (sort_by == "pvalue" && "p_value" %in% names(treated_data)) {
            treated_data <- treated_data[order(p_value)]
        } else if (sort_by == "rmspe_ratio" && "post_pre_ratio" %in% names(treated_data)) {
            treated_data <- treated_data[order(post_pre_ratio)]
        }

        # Create new specification mapping
        treated_data[, new_specification := 1:.N]
        spec_reorder <- treated_data[, .(old_spec = Specification, new_specification)]

        # Apply new ordering to both panel datasets
        panel_a_data <- merge(panel_a_data, spec_reorder, by.x = "Specification", by.y = "old_spec", all.x = TRUE)
        panel_a_data[, Specification := new_specification][, new_specification := NULL]

        panel_b_data <- merge(panel_b_data, spec_reorder, by.x = "Specification", by.y = "old_spec", all.x = TRUE)
        panel_b_data[, Specification := new_specification][, new_specification := NULL]
    }

    # D. Finalize Panel A data
    # Select the null distribution
    null_unit_type <- match.arg(null_distribution, choices = c("placebo", "bootstrap"))
    null_effects <- panel_a_data[unit_type == ifelse(null_unit_type == "placebo", "control", "bootstrap")]

    if (nrow(null_effects) == 0 && null_unit_type == "bootstrap") {
        warning("No bootstrap distribution found. Falling back to placebo units.")
        null_effects <- panel_a_data[unit_type == "control"]
    }

    treated_effects <- panel_a_data[unit_type == "treated"]
    plot_data_p1 <- rbindlist(list(treated_effects, null_effects), use.names = TRUE, fill = TRUE)

    # Set factor levels for plotting order
    plot_data_p1[, unit_type := factor(unit_type, levels = c("treated", "control", "bootstrap"))]

    # E. Finalize Panel B data
    # Clean up feature names for display
    panel_b_data[feature_group=='const', feature_group:= 'Weight\nMethod']
    panel_b_data[feature_group=='outcome', feature_group:=  'Outcome']
    panel_b_data[feature_group=='outcome_model', feature_group:= 'Outcome\nModel']
    panel_b_data[feature_group== 'fw', feature_group:= 'V Weights']
    panel_b_data[feature_group== 'feat', feature_group:= 'Features']
    panel_b_data[feature_group== 'data_sample', feature_group:='Donor Pool']
    panel_b_data[feature_group== 'const', feature_group:= 'Weight\nMethod']
    panel_b_data[feature_group== 'constant', feature_group:= 'Constant\nTerm']
    
    # Transform constraint names for display (preserve descriptive names in Features group)
    panel_b_data[feature == 'simplex' & feature_group=='Weight\nMethod', feature := "Original"]
    panel_b_data[feature == 'lasso' & feature_group=='Weight\nMethod', feature := "Penalty Lasso"]
    panel_b_data[feature == 'ridge' & feature_group=='Weight\nMethod', feature := "Penalty Ridge"]
    panel_b_data[feature == 'ols' & feature_group=='Weight\nMethod', feature := "OLS Weights"]
    
    # Transform constant term display values
    panel_b_data[feature == 'FALSE' & feature_group=='Constant\nTerm', feature := "No Constant"]
    panel_b_data[feature == 'TRUE' & feature_group=='Constant\nTerm', feature := "With Constant"]
    
    # DO NOT transform feature names in the 'Features' group - preserve descriptive names as-is

    # Dynamic feature group detection - only show groups with variation in filtered data
    feature_counts <- panel_b_data[, .(n_unique = data.table::uniqueN(feature)), by = feature_group]
    groups_to_keep <- feature_counts[n_unique > 1, feature_group]
    
    # Validate that feature groups have variation - FAIL HARD if not
    if (length(groups_to_keep) == 0) {
        stop("No feature groups have variation in filtered data. This indicates insufficient specification diversity. ",
             "Available feature groups: ", paste(unique(panel_b_data$feature_group), collapse = ", "), ". ",
             "Either expand your specification parameters or remove restrictive filters.")
    }
    
    message("Displaying feature groups with variation: ", paste(groups_to_keep, collapse = ", "))
    plot_data_p2 <- panel_b_data[feature_group %in% groups_to_keep]


    # --- 2. Create the Final Plots ---
    data.table::setorder(plot_data_p1, -unit_type)

    # Create Panel A with conditional coloring for p-values
    if (has_pvalues && show_pvalues && "p_value" %in% names(plot_data_p1)) {
        treated_data <- plot_data_p1[unit_type == "treated"]
        control_data <- plot_data_p1[unit_type %in% c("control", "bootstrap")]

        pvalue_style='adf'
        if (pvalue_style == "continuous") {
            # Continuous p-value coloring with significance boundary
            treated_data[, is_significant := p_value < p_threshold]

            p1 <- ggplot() +
                geom_point(data = control_data, aes(x = Specification, y = Estimate),
                           color = "gray60", alpha = 0.3, shape = 19, size = 2) +
                # Add significance boundary line for treated effects
                geom_point(data = treated_data[is_significant == TRUE],
                           aes(x = Specification, y = Estimate),
                           color = "black", alpha = 0.9, shape = 21, stroke = 1.2, size = 2) +
                geom_point(data = treated_data, aes(x = Specification, y = Estimate,
                                                    color = -log10(pmax(p_value, 1e-10))),
                           alpha = 0.8, shape = 19, size = 2) +
                geom_hline(yintercept = 0, alpha = 0.5, linetype = 'dashed') +
                scale_color_gradient2(
                    name = "-log10(p-value)",
                    low = "#d73027",      # Red for high p-values (non-significant)
                    mid = "#fee08b",      # Yellow for moderate p-values
                    high = "#1a9850",     # Green for low p-values (significant)
                    midpoint = -log10(p_threshold),   # Threshold at p=0.05
                    breaks = c(0, -log10(c(0.5, 0.1, 0.05, 0.01, 0.001))),
                    labels = c("1.0", "0.5", "0.1", "0.05", "0.01", "0.001"),
                    guide = guide_colorbar(
                        title.position = "top",
                        barwidth = unit(4, "cm"),
                        barheight = unit(0.5, "cm")
                    )
                )
        } else {
            # Categorical p-value coloring (original behavior)
            treated_data[, significance_category := fcase(
                p_value < 0.01, "p < 0.01",
                p_value < 0.05, "p < 0.05",
                p_value < 0.10, "p < 0.10",
                p_value >= 0.10, "p >= 0.10",
                default = "Unknown"
            )]

            p1 <- ggplot() +
                geom_point(data = control_data, aes(x = Specification, y = Estimate),
                           color = "gray60", alpha = 0.3, shape = 19, size = 2) +
                geom_point(data = treated_data, aes(x = Specification, y = Estimate, color = significance_category),
                           alpha = 0.8, shape = 19, size = 2) +
                geom_hline(yintercept = 0, alpha = 0.5, linetype = 'dashed') +
                scale_color_manual(
                    name = "p-value",
                    values = c("p < 0.01" = "#08519c", "p < 0.05" = "#3182bd",
                               "p < 0.10" = "#fd8d3c", "p >= 0.10" = "#d94701",
                               "Unknown" = "#999999"),
                    breaks = c("p < 0.01", "p < 0.05",
                               "p < 0.10", "p >= 0.10")
                )
        }
    } else {
        p1 <- ggplot(plot_data_p1, aes(x = Specification, y = Estimate, fill = unit_type,
                                       color = unit_type, alpha = unit_type)) +
            geom_point(shape = 21, size = 2) +
            geom_hline(yintercept = 0, alpha = 0.5, linetype = 'dashed') +
            scale_fill_manual(name = "Unit Type", values = c(control = "gray60", treated = "#1f78b4", bootstrap = "gray60")) +
            scale_color_manual(name = "Unit Type", values = c(control = "gray60", treated = "#1f78b4", bootstrap = "gray60")) +
            scale_alpha_manual(name = "Unit Type", values = c(treated = 0.8, control = 0.3, bootstrap = 0.3))
    }

    # Add predicted treatment effects if available (BEFORE actual points so they're underneath)
    if (show_predictions && "Predicted" %in% names(plot_data_p1)) {
        # Add predicted values for treated unit only
        treated_predictions <- plot_data_p1[unit_type == "treated" & !is.na(Predicted)]
        
        if (nrow(treated_predictions) > 0) {
            p1 <- p1 +
                # Add subtle connecting lines first (so they're in background)
                geom_segment(data = treated_predictions, 
                           aes(x = Specification, xend = Specification, 
                               y = Estimate, yend = Predicted),
                           color = "red", alpha = 0.2, linetype = "dotted") +
                # Add predicted points (more transparent, underneath actual points)
                geom_point(data = treated_predictions, aes(x = Specification, y = Predicted),
                          color = "red", shape = 4, size = 2.5, stroke = 1.2, alpha = 0.4)
        }
    }
    
    # Re-plot actual treated unit points on top to ensure they're visible
    if (show_predictions && "Predicted" %in% names(plot_data_p1)) {
        treated_actual <- plot_data_p1[unit_type == "treated"]
        if (nrow(treated_actual) > 0) {
            p1 <- p1 +
                geom_point(data = treated_actual, aes(x = Specification, y = Estimate),
                          color = "#1f78b4", shape = 21, size = 2.2, fill = "#1f78b4", alpha = 0.9, stroke = 0.5)
        }
    }

    # Add common elements to both plot types
    p1 <- p1 +

        guides(
            color = guide_legend(title.position = "top", override.aes = list(size = 4))
        ) +

        theme_minimal() +
        theme(
            legend.position = "left",
            legend.box = "vertical",
            axis.line = element_line(color = "black", linewidth = 0.5),
            axis.text = element_text(colour = "black")
        ) +
        labs(x = NULL, y = y_label) +
        # Add subtitle explaining predictions if shown
        {if (show_predictions && "Predicted" %in% names(plot_data_p1))
            labs(subtitle = paste0(y_label, " | Red x = CatBoost predictions (LOO-CV)"))
        else
            NULL
        }

    # Calculate specification curve p-values on filtered data
    # This ensures p-values reflect the actual data being plotted
    spec_curve_pvals <- NULL
    if (is.list(long_data)) {
        # Get expected direction from long_data or default to negative
        expected_direction <- if (!is.null(long_data$expected_direction)) long_data$expected_direction else "negative"

        # Get abadie inference if available
        abadie_inference <- if (!is.null(long_data$abadie_inference)) long_data$abadie_inference else NULL

        # Calculate p-values on the filtered data (sc_results_df after all filtering)
        spec_curve_pvals <- calculate_spec_curve_pvalues_filtered(
            filtered_results = sc_results_df,
            abadie_inference = abadie_inference,
            expected_direction = expected_direction,
            name_treated_unit = name_treated_unit
        )
    }

    # Add specification curve p-values annotation if calculated
    if (!is.null(spec_curve_pvals)) {

        # Extract treated unit p-values for both test statistics
        treated_median_pval <- NULL
        treated_stouffer_pval <- NULL

        # Get median tau p-value for treated unit
        if (!is.null(spec_curve_pvals$median_tau_pvalues)) {
            median_pvals <- spec_curve_pvals$median_tau_pvalues
            treated_median_data <- median_pvals[unit_type == "treated"]
            if (nrow(treated_median_data) > 0) {
                treated_median_pval <- treated_median_data$pval_rank_med[1]
            }
        }

        # Get Stouffer's method p-value for treated unit
        if (!is.null(spec_curve_pvals$stouffer_pvalues) && nrow(spec_curve_pvals$stouffer_pvalues) > 0) {
            stouffer_pvals <- spec_curve_pvals$stouffer_pvalues
            treated_stouffer_data <- stouffer_pvals[unit_type == "treated"]
            if (nrow(treated_stouffer_data) > 0) {
                treated_stouffer_pval <- treated_stouffer_data$pval_rank_z[1]
            }
        }

        # Create annotation text if we have at least one p-value
        if (!is.null(treated_median_pval) || !is.null(treated_stouffer_pval)) {
            annotation_lines <- c()

            if (!is.null(treated_median_pval)) {
                treated_median_rank <- treated_median_data$rank_med[1]
                # Calculate total units from the full median_pvals data
                total_median_units <- nrow(median_pvals)
                annotation_lines <- c(annotation_lines, sprintf("Median tau: p = %.3f (rank %d/%d)", treated_median_pval, treated_median_rank, total_median_units))
            }

            if (!is.null(treated_stouffer_pval)) {
                treated_stouffer_rank <- treated_stouffer_data$rank_z[1]
                # Calculate total units from the full stouffer_pvals data
                total_stouffer_units <- nrow(stouffer_pvals)
                annotation_lines <- c(annotation_lines, sprintf("Stouffer: p = %.3f (rank %d/%d)", treated_stouffer_pval, treated_stouffer_rank, total_stouffer_units))
            }

            spec_annotation_text <- paste(annotation_lines, collapse = "\n")

            # Add annotation to lower-left of Panel A (treatment effects plot)
            p1 <- p1 +
                annotate("text",
                        x = -Inf, y = -Inf,
                        label = spec_annotation_text,
                        hjust = -0.1, vjust = -0.5,   # Left-align and move up slightly
                        size = 3, color = "#000000",
                        fontface = "plain")
        }
    }

    # Apply y-axis limits if outlier cropping is requested
    if (!is.null(y_limits)) {
        p1 <- p1 + ylim(y_limits[1], y_limits[2])
    }

    # Create Panel B (Specification Choices and Shapley Values)
    # Panel B: SHAP visualization with optional significance transparency
    has_shap_significance <- "shap_pvalue" %in% names(plot_data_p2) && "shap_significance_level" %in% names(plot_data_p2)

    # Add SHAP values to feature labels if SHAP values are available
    if ("shapley_value" %in% names(plot_data_p2)) {
        # Calculate SHAP summary values for each individual feature for treated unit
        if (shap_label_type == "absolute") {
            feature_shap_summary <- plot_data_p2[`Unit Name` == name_treated_unit & !is.na(shapley_value), 
                                               .(shap_summary = mean(abs(shapley_value), na.rm = TRUE)), 
                                               by = .(feature_group, feature)]
        } else {  # signed
            feature_shap_summary <- plot_data_p2[`Unit Name` == name_treated_unit & !is.na(shapley_value), 
                                               .(shap_summary = mean(shapley_value, na.rm = TRUE)), 
                                               by = .(feature_group, feature)]
        }
        
        # Get all SHAP values for proper color scaling
        all_treated_shap <- plot_data_p2[`Unit Name` == name_treated_unit & !is.na(shapley_value), shapley_value]
        
        # Map SHAP summary values to colors using the same scale as the legend
        feature_shap_summary[, shap_color := map_shap_to_color(shap_summary, shap_label_type, all_treated_shap)]
        
        # Create enhanced feature labels with colored SHAP values in HTML format
        feature_shap_summary[, feature_with_shap := paste0(
            feature, " (<span style='color:", shap_color, "'>", 
            ifelse(shap_summary >= 0, "+", ""), 
            round(shap_summary, 3), "</span>)"
        )]
        
        # Merge back with plot data to add enhanced labels
        plot_data_p2 <- merge(plot_data_p2, 
                             feature_shap_summary[, .(feature_group, feature, feature_with_shap)], 
                             by = c("feature_group", "feature"), 
                             all.x = TRUE)
        
        # Use enhanced labels where available, fallback to original feature names
        plot_data_p2[, feature_display := ifelse(!is.na(feature_with_shap), feature_with_shap, feature)]
        
        # Sort features alphabetically within each feature group for better readability
        plot_data_p2[, feature_display := factor(feature_display, levels = sort(unique(feature_display)))]
        
        # Create color variable that matches the label type
        if (shap_label_type == "absolute") {
            plot_data_p2[, shap_color_value := abs(shapley_value)]
        } else {  # signed
            plot_data_p2[, shap_color_value := shapley_value]
        }
    } else {
        # No SHAP values - use original feature names
        plot_data_p2[, feature_display := feature]
        
        # Sort features alphabetically within each feature group for better readability
        plot_data_p2[, feature_display := factor(feature_display, levels = sort(unique(feature_display)))]
        
        plot_data_p2[, shap_color_value := NA]  # No SHAP coloring
    }

    # Check if SHAP values are available in the data
    if ("shapley_value" %in% names(plot_data_p2)) {
        # Prepare alpha mapping if significance is available
        if (has_shap_significance) {
            # Create p-value-based alpha mapping for continuous significance scale
            # Lower p-value = more significant = more solid (higher alpha)
            # Map p-values: p=0 -> alpha=1.0 (solid), p=1 -> alpha=0.2 (transparent)
            plot_data_p2[, shap_alpha_value := pmax(0.2, 1.0 - pmin(shap_pvalue, 1.0))]
        }
        
        # Create base plot with appropriate aesthetics
        if (has_shap_significance) {
            p2 <- ggplot(plot_data_p2, aes(x = Specification, y = feature_display)) +
                geom_point(aes(color = shap_color_value, alpha = shap_alpha_value), shape = 15, size = 2.5)
        } else {
            p2 <- ggplot(plot_data_p2, aes(x = Specification, y = feature_display)) +
                geom_point(aes(color = shap_color_value), shape = 15, size = 2.5)
        }
        
        # Add color scale based on shap_label_type
        if (shap_label_type == "absolute") {
            p2 <- p2 + scale_color_gradient(
                name = "|SHAP Value| = Change in Treatment Effect",
                low = SHAP_COLORS$light_gray,
                high = SHAP_COLORS$blue,
                guide = guide_colorbar(title.position = "top")
            )
        } else {  # signed
            p2 <- p2 + scale_color_gradient2(
                name = "SHAP Value = Change in Treatment Effect",
                low = SHAP_COLORS$red,
                mid = SHAP_COLORS$gray,
                high = SHAP_COLORS$blue,
                midpoint = 0,
                guide = guide_colorbar(title.position = "top")
            )
        }
        
        # Add alpha scale if significance is available
        if (has_shap_significance) {
            p2 <- p2 + scale_alpha_continuous(
                name = "SHAP Significance\n(Solid = Low p-value)",
                range = c(0.2, 1.0),
                breaks = c(0.2, 0.5, 0.95, 0.99, 0.999),
                labels = c("p~1.0", "p~0.5", "p~0.05", "p~0.01", "p~0.0")
            )
        }
    } else {
        # No SHAP values - create basic specification plot
        p2 <- ggplot(plot_data_p2, aes(x = Specification, y = feature_display)) +
            geom_point(color = "#666666", shape = 15, size = 2.5)
    }

    # Add remaining plot elements
    p2 <- p2 +
        facet_grid(feature_group ~ ., scales = "free_y", space = "free_y", switch = "y") +
        theme_minimal() +
        theme(
            axis.line.x = element_line(color = "black", linewidth = 0.5),
            axis.text.y = if ("shapley_value" %in% names(plot_data_p2)) {
                ggtext::element_markdown(colour = "black", size = 8)
            } else {
                element_text(colour = "black", size = 8)
            },
            strip.placement = "outside",
            strip.text.y.left = element_text(angle = 0, hjust = 1, face = "bold"),
            panel.spacing = unit(.25, "lines"),
            legend.position = "bottom",
            legend.key.width = unit(1.5, "cm"),
            legend.box = "horizontal"   # Place legends side by side
        ) +
        labs(x = "Specification Number", y = "")


    # --- 3. Combine Plots ---
    final_plot <- plot_grid(
        p1,
        p2,
        ncol = 1,
        align = 'v',
        axis = "lr",
        rel_heights = c(0.4, 0.6)
    )

    # Save plot if file path is provided
    if (!is.na(file_path_save)) {
        ggplot2::ggsave(file_path_save, plot = final_plot, width = width, height = height)
    }

    # Prepare comprehensive return object
    return_object <- list(
        final_plot = final_plot,
        panel_a = p1,
        panel_b = p2,
        plot_data_p1 = plot_data_p1,
        plot_data_p2 = plot_data_p2,
        computed_shap = computed_shap,
        spec_curve_pvals = spec_curve_pvals,
        filtered_specs = nrow(sc_results_df),
        feature_groups_displayed = groups_to_keep
    )
    
    return(return_object)
}

#' Calculate SHAP Significance via Placebo Inference
#'
#' @title Calculate Statistical Significance of SHAP Values
#' @description Performs placebo-style inference on SHAP values by ranking the treated unit's
#' feature importance against control units for each specification and feature group.
#'
#' @param shap_data Data.table. SHAP values for multiple units with columns: unit, full_spec_id,
#'   feature_group, feature, shapley_value
#' @param treated_unit_name Character. Name of the treated unit
#' @param pvalue_method Character. Method for calculating p-values: "absolute" or "signed".
#'   - "absolute": Ranks based on absolute SHAP values.
#'   - "signed": Ranks based on signed SHAP values, distinguishing positive and negative importance.
#'
#' @return Data.table with columns: full_spec_id, feature_group, shap_pvalue, shap_rank,
#'   total_units, shap_significance_level
#'
#' @details
#' For each specification x feature_group combination:
#' \itemize{
#'   \item Extracts relevant SHAP values for all units
#'   \item Ranks units based on the chosen pvalue_method ("absolute" or "signed")
#'   \item Calculates p-value based on treated unit's rank position
#'   \item Assigns significance levels for visualization mapping
#' }
calculate_shap_significance <- function(shap_data, treated_unit_name, pvalue_method = "absolute") { # NEW PARAMETER

    if (!data.table::is.data.table(shap_data)) {
        shap_data <- data.table::as.data.table(shap_data)
    }

    # Input validation for pvalue_method
    valid_pvalue_methods <- c("absolute", "signed")
    if (!pvalue_method %in% valid_pvalue_methods) {
        stop("pvalue_method must be one of: ", paste(valid_pvalue_methods, collapse = ", "))
    }

    # Calculate significance for each specification x feature_group combination
    significance_results <- shap_data[, {

        treated_shap <- shapley_value[unit == treated_unit_name]

        if (length(treated_shap) == 0) {
            # Treated unit not found for this spec x feature_group
            list(
                shap_pvalue = NA_real_,
                shap_rank = NA_integer_,
                total_units = .N,
                shap_significance_level = NA_real_
            )
        } else {
            if (pvalue_method == "absolute") {
                # Original logic: Rank by absolute SHAP value (most important = rank 1)
                shap_rank_val <- NA_integer_
                abs_shap_values <- abs(shapley_value)
                unit_ranks <- rank(-abs_shap_values, ties.method = "min")
                treated_rank <- unit_ranks[unit == treated_unit_name][1]
                p_value <- treated_rank / .N # One-sided test for unusual importance

            } else if (pvalue_method == "signed") {
                # Signed significance: Compare treated unit's signed SHAP to controls
                # This is more complex as it implies a two-sided test around zero,
                # or separate one-sided tests for positive/negative.
                # A common approach for "signed" significance in placebo inference
                # is to rank within the positive values and within the negative values.
                # For simplicity here, we can consider a 'two-tailed' rank against zero.

                # Rank negative SHAP values (more negative = lower rank)
                # Rank positive SHAP values (more positive = lower rank of absolute)
                # Then combine for a two-sided p-value.

                # Let's use a simpler approach that mimics a t-test idea with ranks:
                # How extreme is the treated unit's value compared to the distribution of controls?

                # Rank all values. If a value is positive and large, its rank should be low.
                # If a value is negative and large, its rank should be low.
                # We need to rank by distance from zero for the magnitude, and then consider sign.

                # For signed:
                # 1. Rank units by their signed SHAP values in ascending order.
                # 2. Get the rank of the treated unit.
                # 3. Calculate p-value based on this rank, assuming a null of zero.
                # This is effectively checking if the treated unit's SHAP value is unusually high OR unusually low.

                # Rank based on the actual signed value (ascending rank: smallest value = 1)
                signed_ranks_asc <- rank(shapley_value, ties.method = "average")
                treated_signed_rank_asc <- signed_ranks_asc[unit == treated_unit_name][1]

                # Rank based on the actual signed value (descending rank: largest value = 1)
                signed_ranks_desc <- rank(-shapley_value, ties.method = "average")
                treated_signed_rank_desc <- signed_ranks_desc[unit == treated_unit_name][1]

                # The p-value is the smaller of (rank_asc / N) or (rank_desc / N)
                # This approximates a two-sided p-value based on ranks.
                p_value <- min(treated_signed_rank_asc / .N, treated_signed_rank_desc / .N) * 2 # Multiply by 2 for two-sided test
                p_value <- pmin(p_value, 1.0) # Cap p-value at 1.0

                # For signed significance, the rank `shap_rank` could be less intuitive
                # as it's not simply "most important". We'll just store the one used for p-value.
                shap_rank_val <- min(treated_signed_rank_asc, treated_signed_rank_desc)

            } else {
                # Should not happen due to validation, but as a safeguard
                p_value <- NA_real_
                shap_rank_val <- NA_integer_
            }

            # Create significance level for visualization mapping
            significance_level <- ifelse(p_value < 0.05, 1.0,
                                         ifelse(p_value < 0.10, 0.6, 0.3))

            list(
                shap_pvalue = p_value,
                shap_rank = shap_rank_val, # Use the most extreme rank for reporting
                total_units = .N,
                shap_significance_level = significance_level
            )
        }

    }, by = .(full_spec_id, feature_group)]

    return(significance_results)
}

#' Calculate Specification Curve P-values on Filtered Data
#'
#' @title Calculate Cross-Specification P-values for Filtered Results
#' @description Calculates specification curve-level p-values by aggregating treatment effects
#' across filtered specifications and ranking units. Implements two test statistics:
#' 1) Median treatment effect across specifications, 2) Average Z-score (Stouffer's method).
#' This function operates on the filtered data that will actually be plotted.
#'
#' @param filtered_results Data.table. Filtered results data (post-filtering by outcomes, RMSE, etc.)
#' @param abadie_inference List. Abadie inference results from original spec_curve output
#' @param expected_direction Character. Expected direction of treatment effect: "negative", "positive", or "two_sided"
#' @param name_treated_unit Character. Name of the treated unit for filtering inference results
#'
#' @return List containing specification curve p-values:
#' \itemize{
#'   \item median_tau_pvalues: P-values based on median treatment effect across filtered specifications
#'   \item stouffer_pvalues: P-values based on average Z-score across filtered specifications (Stouffer's method)
#' }
#'
#' @details
#' This function calculates p-values on the subset of data that will actually be visualized,
#' ensuring that the statistical inference matches the displayed results. Key features:
#' \itemize{
#'   \item Works on filtered data (by RMSE threshold, outcomes, etc.)
#'   \item Calculates median treatment effect for each unit across filtered specifications only
#'   \item Converts individual specification p-values to Z-scores and averages them
#'   \item Ranks all units based on filtered results and assigns p-values
#'   \item Accounts for expected direction when ranking (negative vs positive effects)
#' }
calculate_spec_curve_pvalues_filtered <- function(filtered_results, abadie_inference = NULL, expected_direction = "negative", name_treated_unit = NULL) {

    if (nrow(filtered_results) == 0) {
        return(list(
            median_tau_pvalues = data.table(),
            stouffer_pvalues = data.table()
        ))
    }

    # Test Statistic 1: Median Treatment Effect Across Filtered Specifications
    # Calculate average tau for each unit across filtered specs (post-period only)
    avg_tau_by_spec <- filtered_results[post_period == TRUE, .(
        ave_tau = mean(tau)
    ), by = .(full_spec_id, unit_name, unit_type)]

    # Get median of those averages for each unit across filtered specifications
    median_tau_by_unit <- avg_tau_by_spec[, .(
        median_tau = median(ave_tau),
        n_specs = .N
    ), by = .(unit_name, unit_type)]

    # Rank units based on expected direction and assign p-values
    if (expected_direction == "negative") {
        # Most negative gets rank 1 (lowest p-value)
        median_tau_by_unit[, rank_med := rank(median_tau, ties.method = "min")]
    } else if (expected_direction == "positive") {
        # Most positive gets rank 1 (lowest p-value)
        median_tau_by_unit[, rank_med := rank(-median_tau, ties.method = "min")]
    } else {
        # Two-sided: rank by absolute value (most extreme gets rank 1)
        median_tau_by_unit[, rank_med := rank(-abs(median_tau), ties.method = "min")]
    }

    # Calculate p-values
    median_tau_by_unit[, pval_rank_med := rank_med / .N]

    # Test Statistic 2: Average Z-score (Stouffer's Method)
    # Filter Abadie inference to match filtered specifications
    stouffer_results <- data.table()

    if (!is.null(abadie_inference) && "p_values_rmse_ratio" %in% names(abadie_inference)) {

        abadie_pvals <- as.data.table(abadie_inference$p_values_rmse_ratio)

        # Get the specifications that remain after filtering
        filtered_spec_ids <- unique(filtered_results$full_spec_id)

        # Filter Abadie p-values to match filtered specifications
        abadie_pvals_filtered <- abadie_pvals[full_spec_id %in% filtered_spec_ids]

        if (nrow(abadie_pvals_filtered) > 0) {
            # Merge treatment effects with filtered p-values
            tau_pval_merged <- merge(
                avg_tau_by_spec,
                abadie_pvals_filtered[, .(full_spec_id, unit_name, p_value_one_sided)],
                by = c('full_spec_id', 'unit_name'),
                all.x = TRUE
            )

            # Convert p-values to Z-scores
            tau_pval_merged[, zscore := qnorm(p_value_one_sided, mean = 0, sd = 1, lower.tail = TRUE)]

            # Handle extreme p-values
            tau_pval_merged[p_value_one_sided == 1, zscore := 4.25]
            tau_pval_merged[p_value_one_sided == 0, zscore := -4.25]
            tau_pval_merged[is.na(zscore), zscore := 0]    # Handle any remaining NAs

            # Calculate mean Z-score for each unit across filtered specifications
            mean_z_by_unit <- tau_pval_merged[, .(
                mean_z = mean(zscore, na.rm = TRUE),
                n_specs_with_pvals = sum(!is.na(zscore))
            ), by = .(unit_name, unit_type)]

            # Only proceed if we have units with p-values
            if (nrow(mean_z_by_unit) > 0) {
                # Rank units by mean Z-score based on expected direction
                if (expected_direction == "negative") {
                    # Most negative Z-score gets rank 1 (most significant in negative direction)
                    mean_z_by_unit[, rank_z := rank(mean_z, ties.method = "min")]
                } else if (expected_direction == "positive") {
                    # Most positive Z-score gets rank 1 (most significant in positive direction)
                    mean_z_by_unit[, rank_z := rank(-mean_z, ties.method = "min")]
                } else {
                    # Two-sided: most extreme (absolute) Z-score gets rank 1
                    mean_z_by_unit[, rank_z := rank(-abs(mean_z), ties.method = "min")]
                }

                # Calculate p-values
                mean_z_by_unit[, pval_rank_z := rank_z / .N]

                stouffer_results <- mean_z_by_unit
            }
        }
    }

    # Return both test statistics
    return(list(
        median_tau_pvalues = median_tau_by_unit,
        stouffer_pvalues = stouffer_results
    ))
}


